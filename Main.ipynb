{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP12PM4m6X/r6mrKwfZJDIi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshitie2/main/blob/main/L_CBAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjRbawiRMx1Y"
      },
      "outputs": [],
      "source": [
        "#Google Drive Permissions\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rarfile\n",
        "\n",
        "from google.colab import drive\n",
        "import rarfile\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the .rar file in Google Drive\n",
        "rar_path = '/content/drive/MyDrive/Computer Vision/main_data_croped.rar'\n",
        "\n",
        "# Destination folder to extract the contents\n",
        "destination_folder = '/content'\n",
        "\n",
        "# Create the destination folder if it doesn't exist\n",
        "if not os.path.exists(destination_folder):\n",
        "    os.makedirs(destination_folder)\n",
        "\n",
        "# Extract the .rar file\n",
        "with rarfile.RarFile(rar_path, 'r') as rar_ref:\n",
        "    rar_ref.extractall(destination_folder)\n",
        "\n",
        "print(\"Extraction completed.\")"
      ],
      "metadata": {
        "id": "xj0vz4gBakHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convolutional block attention module (CBAM) with  Apply Lambda layer for rescaling the attention results, VGG16\n",
        "!pip install tensorflow-addons\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, Conv2D, Lambda, Reshape, GlobalMaxPooling2D, Average, Multiply, Concatenate, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Set the path to the directory containing the dataset\n",
        "dataset_dir = '/content/main_data_croped'\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(os.path.join(dataset_dir, '/content/drive/MyDrive/Computer Vision/tfti2.csv'), usecols=[\"key\", \"class\"])\n",
        "\n",
        "# Convert the 'class' column to string\n",
        "data['class'] = data['class'].astype(str)\n",
        "\n",
        "# Filter the data to include only classes 1 and 2\n",
        "data = data[data['class'].isin(['1', '2', '3'])]\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=0)\n",
        "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=0)\n",
        "\n",
        "# Print the number of samples in each set\n",
        "print('Number of train samples:', train_data.shape[0])\n",
        "print('Number of valid samples:', valid_data.shape[0])\n",
        "print('Number of test samples:', test_data.shape[0])\n",
        "\n",
        "# Preprocess data\n",
        "train_data[\"key\"] = train_data[\"key\"].apply(lambda x: x + \".jpg\")\n",
        "valid_data[\"key\"] = valid_data[\"key\"].apply(lambda x: x + \".jpg\")\n",
        "test_data[\"key\"] = test_data[\"key\"].apply(lambda x: x + \".jpg\")\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "HEIGHT = 224\n",
        "WIDTH = 224\n",
        "N_CLASSES = 3\n",
        "\n",
        "# Create data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_data,\n",
        "    directory=dataset_dir,\n",
        "    x_col=\"key\",\n",
        "    y_col=\"class\",\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    target_size=(HEIGHT, WIDTH),\n",
        "    seed=0\n",
        ")\n",
        "\n",
        "valid_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_dataframe(\n",
        "    dataframe=valid_data,\n",
        "    directory=dataset_dir,\n",
        "    x_col=\"key\",\n",
        "    y_col=\"class\",\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    target_size=(HEIGHT, WIDTH),\n",
        "    seed=0\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_data,\n",
        "    directory=dataset_dir,\n",
        "    x_col=\"key\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False,\n",
        "    target_size=(HEIGHT, WIDTH),\n",
        "    seed=0\n",
        ")\n",
        "\n",
        "# Load the VGG16 model\n",
        "vgg_model = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(HEIGHT, WIDTH, 3)\n",
        ")\n",
        "\n",
        "# Channel attention module\n",
        "x = vgg_model.output\n",
        "avg_pool = GlobalAveragePooling2D()(x)\n",
        "max_pool = GlobalMaxPooling2D()(x)\n",
        "avg_pool = Dense(512)(avg_pool)\n",
        "max_pool = Dense(512)(max_pool)\n",
        "avg_pool = Activation('relu')(avg_pool)\n",
        "max_pool = Activation('relu')(max_pool)\n",
        "channel_attention = Average()([avg_pool, max_pool])\n",
        "channel_attention = Dense(512)(channel_attention)\n",
        "channel_attention = Activation('sigmoid')(channel_attention)\n",
        "channel_attention = Reshape((1, 1, 512))(channel_attention)\n",
        "channel_attention = Lambda(lambda x: x / tf.reduce_max(x))(channel_attention)\n",
        "x = Multiply()([x, channel_attention])\n",
        "\n",
        "# Spatial attention module\n",
        "avg_pool = Lambda(lambda x: tf.keras.backend.mean(x, axis=3, keepdims=True))(x)\n",
        "max_pool = Lambda(lambda x: tf.keras.backend.max(x, axis=3, keepdims=True))(x)\n",
        "concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
        "spatial_attention = Conv2D(filters=1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n",
        "#spatial_attention = Lambda(lambda x: x / tf.reduce_max(x))(spatial_attention)\n",
        "x = Multiply()([x, spatial_attention])\n",
        "\n",
        "# Build the model architecture\n",
        "input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
        "x = vgg_model(input_tensor)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)  # Add dropout regularization\n",
        "x = Dense(N_CLASSES, activation='softmax', kernel_regularizer=l2(0.01), name='output')(x)\n",
        "model_vgg3 = Model(inputs=input_tensor, outputs=x)\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model_vgg3.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy', # binary_crossentropy for two class\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Learning Rate Scheduling\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-7)\n",
        "\n",
        "# Fine-tune the model\n",
        "EPOCHS = 50\n",
        "history_model3 = model_vgg3.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_generator,\n",
        "    callbacks=[reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_vgg3.evaluate(\n",
        "    test_generator,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)"
      ],
      "metadata": {
        "id": "KZnhUj30af3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Extract features using pre-trained model\n",
        "predictions = model_vgg3.layers[-2].output\n",
        "model_feat = keras.Model(inputs=model_vgg3.inputs, outputs=predictions)\n",
        "Extracted_features = model_feat.predict(test_generator)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_image, valid_image, train_label, valid_label = train_test_split(Extracted_features, test_generator.classes,test_size=0.20, stratify=test_generator.classes, random_state=10)\n",
        "val_img, test_img, val_lab, test_lab = train_test_split(valid_image, valid_label, test_size=0.5, stratify=valid_label, random_state=50)\n",
        "\n",
        "# Train MLP classifier\n",
        "_classifier = MLPClassifier(hidden_layer_sizes=(64,), activation='relu', solver='adam', max_iter=1000)\n",
        "_classifier.fit(train_image, train_label)\n",
        "\n",
        "# Evaluate performance on test data\n",
        "pred = _classifier.predict(test_img)\n",
        "acc = accuracy_score(y_true=test_lab, y_pred=pred)\n",
        "print('Test accuracy:', acc)\n",
        "\n",
        "# Save extracted features and labels as CSV files\n",
        "final_features = Extracted_features\n",
        "lab = np.array(test_generator.classes)\n",
        "lab = lab.reshape(lab.shape[0], 1)\n",
        "np.savetxt(\"ext_features.csv\", final_features, delimiter=\",\")\n",
        "np.savetxt(\"labels.csv\", lab, delimiter=\",\")\n",
        "\n",
        "# Load and prepare data for training a new model\n",
        "df = pd.read_csv('/content/ext_features.csv', header=None)\n",
        "df2 = pd.read_csv('/content/labels.csv', header=None)\n",
        "\n",
        "total_features = df.shape[1]\n",
        "x = df[df.columns[:total_features]]\n",
        "y = df2[df2.columns[-1]].astype(int)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size=0.2, stratify=y, random_state=10)\n",
        "\n",
        "print('Training data shape:',train_x.shape)\n",
        "print('Training labels shape:', train_y.shape)\n",
        "print('Testing data shape:', test_x.shape)\n",
        "print('Testing labels shape:', test_y.shape)"
      ],
      "metadata": {
        "id": "zE7mQSBweCV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SSD without ABHC\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "swarm_size = 20   #population size\n",
        "max_iterations = 100\n",
        "omega = 0.2  #used in the fitness function\n",
        "delta=0.2   #to set an upper limit for including a slightly worse particle in LAHC\n",
        "\n",
        "def mutate(agent):\n",
        "    percent=0.2\n",
        "    numChange=int(total_features*percent)\n",
        "    pos=np.random.randint(0,total_features-1,numChange) #choose random positions to be mutated\n",
        "    agent[pos]=1-agent[pos] #mutation\n",
        "    return agent\n",
        "\n",
        "def find_fitness(particle):\n",
        "    features = []\n",
        "    for x in range(len(particle)):\n",
        "        if particle[x]>=0.5: #convert it to zeros and ones\n",
        "            features.append(df.columns[x])\n",
        "    if(len(features)==0):\n",
        "        return 10000\n",
        "    new_x_train = train_x[features].copy()\n",
        "    new_x_test = test_x[features].copy()\n",
        "\n",
        "    _classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "    _classifier.fit(new_x_train, train_y)\n",
        "    predictions = _classifier.predict(new_x_test)\n",
        "    acc = accuracy_score(y_true = test_y, y_pred = predictions)\n",
        "    fitness = acc\n",
        "    err=1-acc\n",
        "    num_features = len(features)\n",
        "    fitness =  alpha*err + (1-alpha)*(num_features/total_features)\n",
        "\n",
        "    return fitness\n",
        "\n",
        "def transfer_func(velocity): #to convert into an array of zeros and ones\n",
        "    t=[]\n",
        "    for i in range(len(velocity)):\n",
        "        t.append(abs(velocity[i]/(math.sqrt(1+velocity[i]*velocity[i])))) #transfer function inside paranthesis\n",
        "    return t\n",
        "\n",
        "#initialize swarm position and swarm velocity of SSD\n",
        "swarm_vel = np.random.uniform(low=0, high=1, size=(swarm_size,total_features))\n",
        "\n",
        "swarm_pos = np.random.uniform(size=(swarm_size,total_features))\n",
        "swarm_pos = np.where(swarm_pos>=0.5,1,0)\n",
        "\n",
        "c = 100\n",
        "alpha= 0.9\n",
        "\n",
        "gbest_fitness=100000\n",
        "pbest_fitness = np.zeros(swarm_size)\n",
        "pbest_fitness.fill(np.inf)  #initialize with the worse possible values\n",
        "pbest = np.empty((swarm_size,total_features))\n",
        "gbest = np.empty(total_features)\n",
        "pbest.fill(np.inf)\n",
        "gbest.fill(np.inf)\n",
        "\n",
        "for itr in range(max_iterations):\n",
        "\n",
        "    for i in range(swarm_size):\n",
        "\n",
        "        fitness = find_fitness(swarm_pos[i])\n",
        "\n",
        "        if fitness < gbest_fitness:\n",
        "\n",
        "            gbest=swarm_pos[i].copy() #updating global best\n",
        "            gbest_fitness=fitness\n",
        "\n",
        "        if fitness < pbest_fitness[i]:\n",
        "            pbest[i] = swarm_pos[i].copy() #updating personal best\n",
        "            pbest_fitness[i]=fitness\n",
        "\n",
        "        r1 = random.random()\n",
        "        r2 = random.random()\n",
        "\n",
        "        #updating the swarm velocity\n",
        "        if r1 < 0.5:\n",
        "            swarm_vel[i] = c*math.sin(r2)*(pbest[i]-swarm_pos[i]) +math.sin(r2)* (gbest-swarm_pos[i])\n",
        "        else:\n",
        "            swarm_vel[i] = c*math.cos(r2)*(pbest[i]-swarm_pos[i]) + math.cos(r2)*(gbest-swarm_pos[i])\n",
        "\n",
        "        #decaying value of c\n",
        "        alpha= 0.9\n",
        "        c=alpha*c;\n",
        "\n",
        "        #applying transfer function and then updating the swarm position\n",
        "        t = transfer_func(swarm_vel[i])\n",
        "        for j in range(len(swarm_pos[i])):\n",
        "            if(t[j] < 0.5):\n",
        "                swarm_pos[i][j] = swarm_pos[i][j]\n",
        "            else:\n",
        "                swarm_pos[i][j] = 1 - swarm_pos[i][j]\n",
        "\n",
        "selected_features = gbest\n",
        "print(gbest_fitness)\n",
        "\n",
        "number_of_selected_features = np.sum(selected_features)\n",
        "print(\"#\",number_of_selected_features)\n",
        "\n",
        "features=[]\n",
        "for j in range(len(selected_features)):\n",
        "    if selected_features[j]==1:\n",
        "        features.append(df.columns[j])\n",
        "new_x_train = train_x[features]\n",
        "new_x_test = test_x[features]\n",
        "\n",
        "_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "_classifier.fit(new_x_train, train_y)\n",
        "predictions = _classifier.predict(new_x_test)\n",
        "acc = accuracy_score(y_true = test_y, y_pred = predictions)\n",
        "pre = precision_score(y_true = test_y, y_pred = predictions,average=None)\n",
        "rec = recall_score(y_true = test_y, y_pred = predictions, average=None)\n",
        "result = classification_report(y_true=test_y, y_pred=predictions)\n",
        "\n",
        "print(\"Accuracy: \", acc)\n",
        "print(\"Precision: \", pre)\n",
        "print(\"Recall: \", rec)\n",
        "print(\"Classification Report: \\n\", result)"
      ],
      "metadata": {
        "id": "Ewi_M5uueNkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using ABHC\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "swarm_size = 20   #population size\n",
        "max_iterations = 100\n",
        "omega = 0.2  #used in the fitness function\n",
        "delta=0.2   #to set an upper limit for including a slightly worse particle in LAHC\n",
        "\n",
        "\n",
        "def mutate(agent):\n",
        "    percent=0.2\n",
        "    numChange=int(total_features*percent)\n",
        "    pos=np.random.randint(0,total_features-1,numChange) #choose random positions to be mutated\n",
        "    agent[pos]=1-agent[pos] #mutation\n",
        "    return agent\n",
        "\n",
        "def ABHC(particle):\n",
        "    _lambda = 15 #upper limit on number of iterations in LAHC\n",
        "    target_fitness = find_fitness(particle) #original fitness\n",
        "    for i in range(_lambda):\n",
        "        new_particle = mutate(particle) #first mutation\n",
        "        temp = find_fitness(new_particle)\n",
        "        if temp < target_fitness:\n",
        "            particle = new_particle.copy() #updation\n",
        "            target_fitness = temp\n",
        "        elif (temp<=(1+delta)*target_fitness):\n",
        "            temp_particle = new_particle.copy()\n",
        "            for j in range(_lambda):\n",
        "                temp_particle1 = mutate(temp_particle) #second mutation\n",
        "                temp_fitness = find_fitness(temp_particle1)\n",
        "                if temp_fitness < target_fitness:\n",
        "                    target_fitness=temp_fitness\n",
        "                    particle=temp_particle1.copy() #updation\n",
        "                break\n",
        "    return particle\n",
        "\n",
        "def randomwalk(agent):\n",
        "    percent = 30\n",
        "    percent /= 100\n",
        "    neighbor = agent.copy()\n",
        "    size = np.shape(agent)[0]\n",
        "    upper = int(percent*size)\n",
        "    if upper <= 1:\n",
        "        upper = size\n",
        "    x = random.randint(1,upper)\n",
        "    pos = random.sample(range(0,size - 1),x)\n",
        "    for i in pos:\n",
        "        neighbor[i] = 1 - neighbor[i]\n",
        "    return neighbor\n",
        "\n",
        "def adaptiveBeta(agent):\n",
        "    bmin = 0.1 #parameter: (can be made 0.01)\n",
        "    bmax = 1\n",
        "    maxIter = 10 # parameter: (can be increased )\n",
        "\n",
        "    agentFit = find_fitness(agent)\n",
        "    for curr in range(maxIter):\n",
        "        neighbor = agent.copy()\n",
        "        size = np.shape(neighbor)[0]\n",
        "        neighbor = randomwalk(neighbor)\n",
        "\n",
        "        beta = bmin + (curr / maxIter)*(bmax - bmin)\n",
        "        for i in range(size):\n",
        "            random.seed( time.time() + i )\n",
        "            if random.random() <= beta:\n",
        "                neighbor[i] = agent[i].copy()\n",
        "        neighFit = find_fitness(neighbor)\n",
        "        if neighFit <= agentFit:\n",
        "            agent = neighbor.copy()\n",
        "\n",
        "    return agent\n",
        "\n",
        "def find_fitness(particle):\n",
        "    features = []\n",
        "    for x in range(len(particle)):\n",
        "        if particle[x]>=0.5: #convert it to zeros and ones\n",
        "            features.append(df.columns[x])\n",
        "    if(len(features)==0):\n",
        "        return 10000\n",
        "    new_x_train = train_x[features].copy()\n",
        "    new_x_test = test_x[features].copy()\n",
        "\n",
        "    _classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "    _classifier.fit(new_x_train, train_y)\n",
        "    predictions = _classifier.predict(new_x_test)\n",
        "    acc = accuracy_score(y_true = test_y, y_pred = predictions)\n",
        "    fitness = acc\n",
        "    err=1-acc\n",
        "    num_features = len(features)\n",
        "    fitness =  alpha*err + (1-alpha)*(num_features/total_features)\n",
        "\n",
        "    return fitness\n",
        "\n",
        "def transfer_func(velocity): #to convert into an array of zeros and ones\n",
        "    t=[]\n",
        "    for i in range(len(velocity)):\n",
        "        t.append(abs(velocity[i]/(math.sqrt(1+velocity[i]*velocity[i])))) #transfer function inside paranthesis\n",
        "    return t\n",
        "\n",
        "#initialize swarm position and swarm velocity of SSD\n",
        "swarm_vel = np.random.uniform(low=0, high=1, size=(swarm_size,total_features))\n",
        "\n",
        "swarm_pos = np.random.uniform(size=(swarm_size,total_features))\n",
        "swarm_pos = np.where(swarm_pos>=0.5,1,0)\n",
        "\n",
        "c = 100\n",
        "alpha= 0.9\n",
        "\n",
        "gbest_fitness=100000\n",
        "pbest_fitness = np.zeros(swarm_size)\n",
        "pbest_fitness.fill(np.inf)  #initialize with the worse possible values\n",
        "pbest = np.empty((swarm_size,total_features))\n",
        "gbest = np.empty(total_features)\n",
        "pbest.fill(np.inf)\n",
        "gbest.fill(np.inf)\n",
        "\n",
        "for itr in range(max_iterations):\n",
        "\n",
        "    for i in range(swarm_size):\n",
        "\n",
        "        swarm_pos[i] = adaptiveBeta(swarm_pos[i]) #for ABHC local search\n",
        "        #swarm_pos[i] = LAHC(swarm_pos[i]) #for LAHC local search\n",
        "        fitness = find_fitness(swarm_pos[i])\n",
        "\n",
        "        if fitness < gbest_fitness:\n",
        "\n",
        "            gbest=swarm_pos[i].copy() #updating global best\n",
        "            gbest_fitness=fitness\n",
        "\n",
        "        if fitness < pbest_fitness[i]:\n",
        "            pbest[i] = swarm_pos[i].copy() #updating personal best\n",
        "            pbest_fitness[i]=fitness\n",
        "\n",
        "        r1 = random.random()\n",
        "        r2 = random.random()\n",
        "\n",
        "        #updating the swarm velocity\n",
        "        if r1 < 0.5:\n",
        "            swarm_vel[i] = c*math.sin(r2)*(pbest[i]-swarm_pos[i]) +math.sin(r2)* (gbest-swarm_pos[i])\n",
        "        else:\n",
        "            swarm_vel[i] = c*math.cos(r2)*(pbest[i]-swarm_pos[i]) + math.cos(r2)*(gbest-swarm_pos[i])\n",
        "\n",
        "        #decaying value of c\n",
        "        alpha= 0.9\n",
        "        c=alpha*c;\n",
        "\n",
        "        #applying transfer function and then updating the swarm position\n",
        "        t = transfer_func(swarm_vel[i])\n",
        "        for j in range(len(swarm_pos[i])):\n",
        "            if(t[j] < 0.5):\n",
        "                swarm_pos[i][j] = swarm_pos[i][j]\n",
        "            else:\n",
        "                swarm_pos[i][j] = 1 - swarm_pos[i][j]\n",
        "\n",
        "selected_features = gbest\n",
        "print(gbest_fitness)\n",
        "\n",
        "number_of_selected_features = np.sum(selected_features)\n",
        "print(\"#\",number_of_selected_features)\n",
        "\n",
        "features=[]\n",
        "for j in range(len(selected_features)):\n",
        "    if selected_features[j]==1:\n",
        "        features.append(df.columns[j])\n",
        "new_x_train = train_x[features]\n",
        "new_x_test = test_x[features]\n",
        "\n",
        "_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "_classifier.fit(new_x_train, train_y)\n",
        "predictions = _classifier.predict(new_x_test)\n",
        "acc = accuracy_score(y_true = test_y, y_pred = predictions)\n",
        "pre = precision_score(y_true = test_y, y_pred = predictions,average=None)\n",
        "rec = recall_score(y_true = test_y, y_pred = predictions, average=None)\n",
        "result = classification_report(y_true = test_y, y_pred = predictions, digits=5)\n",
        "fitness = acc\n",
        "print(\"Acc:\",fitness)\n",
        "print(\"Precision:\", pre)\n",
        "print(\"Recall:\",rec)\n",
        "print(result)\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "n3Q-rW-teZyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using LAHC\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "swarm_size = 20   #population size\n",
        "max_iterations = 100\n",
        "omega = 0.2  #used in the fitness function\n",
        "delta=0.2   #to set an upper limit for including a slightly worse particle in LAHC\n",
        "\n",
        "\n",
        "def mutate(agent):\n",
        "    percent=0.2\n",
        "    numChange=int(total_features*percent)\n",
        "    pos=np.random.randint(0,total_features-1,numChange) #choose random positions to be mutated\n",
        "    agent[pos]=1-agent[pos] #mutation\n",
        "    return agent\n",
        "\n",
        "def LAHC(particle):\n",
        "    _lambda = 15 #upper limit on number of iterations in LAHC\n",
        "    target_fitness = find_fitness(particle) #original fitness\n",
        "    for i in range(_lambda):\n",
        "        new_particle = mutate(particle) #first mutation\n",
        "        temp = find_fitness(new_particle)\n",
        "        if temp < target_fitness:\n",
        "            particle = new_particle.copy() #updation\n",
        "            target_fitness = temp\n",
        "        elif (temp<=(1+delta)*target_fitness):\n",
        "            temp_particle = new_particle.copy()\n",
        "            for j in range(_lambda):\n",
        "                temp_particle1 = mutate(temp_particle) #second mutation\n",
        "                temp_fitness = find_fitness(temp_particle1)\n",
        "                if temp_fitness < target_fitness:\n",
        "                    target_fitness=temp_fitness\n",
        "                    particle=temp_particle1.copy() #updation\n",
        "                break\n",
        "    return particle\n",
        "\n",
        "def randomwalk(agent):\n",
        "    percent = 30\n",
        "    percent /= 100\n",
        "    neighbor = agent.copy()\n",
        "    size = np.shape(agent)[0]\n",
        "    upper = int(percent*size)\n",
        "    if upper <= 1:\n",
        "        upper = size\n",
        "    x = random.randint(1,upper)\n",
        "    pos = random.sample(range(0,size - 1),x)\n",
        "    for i in pos:\n",
        "        neighbor[i] = 1 - neighbor[i]\n",
        "    return neighbor\n",
        "\n",
        "def adaptiveBeta(agent):\n",
        "    bmin = 0.1 #parameter: (can be made 0.01)\n",
        "    bmax = 1\n",
        "    maxIter = 10 # parameter: (can be increased )\n",
        "\n",
        "    agentFit = find_fitness(agent)\n",
        "    for curr in range(maxIter):\n",
        "        neighbor = agent.copy()\n",
        "        size = np.shape(neighbor)[0]\n",
        "        neighbor = randomwalk(neighbor)\n",
        "\n",
        "        beta = bmin + (curr / maxIter)*(bmax - bmin)\n",
        "        for i in range(size):\n",
        "            random.seed( time.time() + i )\n",
        "            if random.random() <= beta:\n",
        "                neighbor[i] = agent[i].copy()\n",
        "        neighFit = find_fitness(neighbor)\n",
        "        if neighFit <= agentFit:\n",
        "            agent = neighbor.copy()\n",
        "\n",
        "    return agent\n",
        "\n",
        "def find_fitness(particle):\n",
        "    features = []\n",
        "    for x in range(len(particle)):\n",
        "        if particle[x]>=0.5: #convert it to zeros and ones\n",
        "            features.append(df.columns[x])\n",
        "    if(len(features)==0):\n",
        "        return 10000\n",
        "    new_x_train = train_x[features].copy()\n",
        "    new_x_test = test_x[features].copy()\n",
        "\n",
        "    _classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "    _classifier.fit(new_x_train, train_y)\n",
        "    predictions = _classifier.predict(new_x_test)\n",
        "    acc = accuracy_score(y_true = test_y, y_pred = predictions)\n",
        "    fitness = acc\n",
        "    err=1-acc\n",
        "    num_features = len(features)\n",
        "    fitness =  alpha*err + (1-alpha)*(num_features/total_features)\n",
        "\n",
        "    return fitness\n",
        "\n",
        "def transfer_func(velocity): #to convert into an array of zeros and ones\n",
        "    t=[]\n",
        "    for i in range(len(velocity)):\n",
        "        t.append(abs(velocity[i]/(math.sqrt(1+velocity[i]*velocity[i])))) #transfer function inside paranthesis\n",
        "    return t\n",
        "\n",
        "#initialize swarm position and swarm velocity of SSD\n",
        "swarm_vel = np.random.uniform(low=0, high=1, size=(swarm_size,total_features))\n",
        "\n",
        "swarm_pos = np.random.uniform(size=(swarm_size,total_features))\n",
        "swarm_pos = np.where(swarm_pos>=0.5,1,0)\n",
        "\n",
        "c = 100\n",
        "alpha= 0.9\n",
        "\n",
        "gbest_fitness=100000\n",
        "pbest_fitness = np.zeros(swarm_size)\n",
        "pbest_fitness.fill(np.inf)  #initialize with the worse possible values\n",
        "pbest = np.empty((swarm_size,total_features))\n",
        "gbest = np.empty(total_features)\n",
        "pbest.fill(np.inf)\n",
        "gbest.fill(np.inf)\n",
        "\n",
        "for itr in range(max_iterations):\n",
        "\n",
        "    for i in range(swarm_size):\n",
        "\n",
        "        #swarm_pos[i] = adaptiveBeta(swarm_pos[i]) #for ABHC local search\n",
        "        swarm_pos[i] = LAHC(swarm_pos[i]) #for LAHC local search\n",
        "        fitness = find_fitness(swarm_pos[i])\n",
        "\n",
        "        if fitness < gbest_fitness:\n",
        "\n",
        "            gbest=swarm_pos[i].copy() #updating global best\n",
        "            gbest_fitness=fitness\n",
        "\n",
        "        if fitness < pbest_fitness[i]:\n",
        "            pbest[i] = swarm_pos[i].copy() #updating personal best\n",
        "            pbest_fitness[i]=fitness\n",
        "\n",
        "        r1 = random.random()\n",
        "        r2 = random.random()\n",
        "\n",
        "        #updating the swarm velocity\n",
        "        if r1 < 0.5:\n",
        "            swarm_vel[i] = c*math.sin(r2)*(pbest[i]-swarm_pos[i]) +math.sin(r2)* (gbest-swarm_pos[i])\n",
        "        else:\n",
        "            swarm_vel[i] = c*math.cos(r2)*(pbest[i]-swarm_pos[i]) + math.cos(r2)*(gbest-swarm_pos[i])\n",
        "\n",
        "        #decaying value of c\n",
        "        alpha= 0.9\n",
        "        c=alpha*c;\n",
        "\n",
        "        #applying transfer function and then updating the swarm position\n",
        "        t = transfer_func(swarm_vel[i])\n",
        "        for j in range(len(swarm_pos[i])):\n",
        "            if(t[j] < 0.5):\n",
        "                swarm_pos[i][j] = swarm_pos[i][j]\n",
        "            else:\n",
        "                swarm_pos[i][j] = 1 - swarm_pos[i][j]\n",
        "\n",
        "selected_features = gbest\n",
        "print(gbest_fitness)\n",
        "\n",
        "number_of_selected_features = np.sum(selected_features)\n",
        "print(\"#\",number_of_selected_features)\n",
        "\n",
        "features=[]\n",
        "for j in range(len(selected_features)):\n",
        "    if selected_features[j]==1:\n",
        "        features.append(df.columns[j])\n",
        "new_x_train = train_x[features]\n",
        "new_x_test = test_x[features]\n",
        "\n",
        "_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "_classifier.fit(new_x_train, train_y)\n",
        "predictions = _classifier.predict(new_x_test)\n",
        "acc = accuracy_score(y_true = test_y, y_pred = predictions)\n",
        "pre = precision_score(y_true = test_y, y_pred = predictions,average=None)\n",
        "rec = recall_score(y_true = test_y, y_pred = predictions, average=None)\n",
        "result = classification_report(y_true = test_y, y_pred = predictions, digits=5)\n",
        "fitness = acc\n",
        "print(\"Acc:\",fitness)\n",
        "print(\"Precision:\", pre)\n",
        "print(\"Recall:\",rec)\n",
        "print(result)\n",
        "print(\"\\n\\n\")"
      ],
      "metadata": {
        "id": "mRjuy1_femPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbconvert"
      ],
      "metadata": {
        "id": "1BGd1mYwe45n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to=python Comparison_algorithms/GA.ipynb # first upload EO.ipynb in Colab environment in Comparison_algorithms folder\n",
        "!jupyter nbconvert --to=python Comparison_algorithms/GSA.ipynb\n",
        "!jupyter nbconvert --to=python Comparison_algorithms/GWO.ipynb\n",
        "!jupyter nbconvert --to=python Comparison_algorithms/WOA.ipynb\n",
        "!jupyter nbconvert --to=python Comparison_algorithms/PSO.ipynb\n",
        "!jupyter nbconvert --to=python Comparison_algorithms/SCA.ipynb\n",
        "!jupyter nbconvert --to=python Comparison_algorithms/EO.ipynb\n",
        "!jupyter nbconvert --to=python Comparison_algorithms/HS.ipynb\n"
      ],
      "metadata": {
        "id": "X9Po8-y6e51W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Comparison_algorithms.GA import GA # for error copy below confusion matric code and pased by oping link and poast on utilities.py\n",
        "from Comparison_algorithms.GSA import GSA\n",
        "from Comparison_algorithms.GWO import GWO\n",
        "from Comparison_algorithms.WOA import WOA\n",
        "from Comparison_algorithms.PSO import PSO\n",
        "from Comparison_algorithms.SCA import SCA\n",
        "from Comparison_algorithms.EO import EO\n",
        "from Comparison_algorithms.HS import HS"
      ],
      "metadata": {
        "id": "WyAbXGxVe81A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#solution, result = GA(20, 100, x, y, save_conv_graph=True)   # To run Local search based GA algorithm\n",
        "#solution, result = GSA(20, 100, x, y, save_conv_graph=True)  # To run Local search based GSA algorithm\n",
        "#solution, result = GWO(20, 100, x, y, save_conv_graph=True)  # To run Local search based GWO algorithm\n",
        "#solution, result = WOA(20, 100, x, y, save_conv_graph=True)   # To run Local search based WOA algorithm\n",
        "#solution, result = PSO(20, 100, x, y, save_conv_graph=True)  # To run Local search based PSO algorithm\n",
        "#solution, result = SCA(20, 100, x, y, save_conv_graph=True)  # To run Local search based SCA algorithm\n",
        "solution, result = EO(20, 100, x, y, save_conv_graph=True)   # To run Local search based EO algorithm\n",
        "#1solution, result = HS(20, 100, x, y, save_conv_graph=True)   # To run Local search based HS algorithm\n",
        "print(result.accuracy)\n",
        "print(result.precision)\n",
        "print(result.recall)"
      ],
      "metadata": {
        "id": "NxJ8HfH8fC7Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}