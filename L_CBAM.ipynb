{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1+lqsnrtl45iMDgoXq7zu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshitie2/main/blob/main/L_CBAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjRbawiRMx1Y"
      },
      "outputs": [],
      "source": [
        "#convolutional block attention module (CBAM) with  Apply Lambda layer for rescaling the attention results, VGG16\n",
        "!pip install tensorflow-addons\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, Conv2D, Lambda, Reshape, GlobalMaxPooling2D, Average, Multiply, Concatenate, Activation\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Set the path to the directory containing the dataset\n",
        "dataset_dir = '/content/main_data_croped'\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(os.path.join(dataset_dir, '/content/drive/MyDrive/computer_vision/tfti2.csv'), usecols=[\"key\", \"class\"])\n",
        "\n",
        "# Convert the 'class' column to string\n",
        "data['class'] = data['class'].astype(str)\n",
        "\n",
        "# Filter the data to include only classes 1 and 2\n",
        "data = data[data['class'].isin(['1', '2', '3'])]\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=0)\n",
        "valid_data, test_data = train_test_split(test_data, test_size=0.5, random_state=0)\n",
        "\n",
        "# Print the number of samples in each set\n",
        "print('Number of train samples:', train_data.shape[0])\n",
        "print('Number of valid samples:', valid_data.shape[0])\n",
        "print('Number of test samples:', test_data.shape[0])\n",
        "\n",
        "# Preprocess data\n",
        "train_data[\"key\"] = train_data[\"key\"].apply(lambda x: x + \".jpg\")\n",
        "valid_data[\"key\"] = valid_data[\"key\"].apply(lambda x: x + \".jpg\")\n",
        "test_data[\"key\"] = test_data[\"key\"].apply(lambda x: x + \".jpg\")\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "HEIGHT = 224\n",
        "WIDTH = 224\n",
        "N_CLASSES = 3\n",
        "\n",
        "# Create data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_data,\n",
        "    directory=dataset_dir,\n",
        "    x_col=\"key\",\n",
        "    y_col=\"class\",\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    target_size=(HEIGHT, WIDTH),\n",
        "    seed=0\n",
        ")\n",
        "\n",
        "valid_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True\n",
        ")\n",
        "\n",
        "valid_generator = valid_datagen.flow_from_dataframe(\n",
        "    dataframe=valid_data,\n",
        "    directory=dataset_dir,\n",
        "    x_col=\"key\",\n",
        "    y_col=\"class\",\n",
        "    class_mode=\"categorical\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    target_size=(HEIGHT, WIDTH),\n",
        "    seed=0\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_data,\n",
        "    directory=dataset_dir,\n",
        "    x_col=\"key\",\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False,\n",
        "    target_size=(HEIGHT, WIDTH),\n",
        "    seed=0\n",
        ")\n",
        "\n",
        "# Load the VGG16 model\n",
        "vgg_model = VGG16(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(HEIGHT, WIDTH, 3)\n",
        ")\n",
        "\n",
        "# Channel attention module\n",
        "x = vgg_model.output\n",
        "avg_pool = GlobalAveragePooling2D()(x)\n",
        "max_pool = GlobalMaxPooling2D()(x)\n",
        "avg_pool = Dense(512)(avg_pool)\n",
        "max_pool = Dense(512)(max_pool)\n",
        "avg_pool = Activation('relu')(avg_pool)\n",
        "max_pool = Activation('relu')(max_pool)\n",
        "channel_attention = Average()([avg_pool, max_pool])\n",
        "channel_attention = Dense(512)(channel_attention)\n",
        "channel_attention = Activation('sigmoid')(channel_attention)\n",
        "channel_attention = Reshape((1, 1, 512))(channel_attention)\n",
        "channel_attention = Lambda(lambda x: x / tf.reduce_max(x))(channel_attention)\n",
        "x = Multiply()([x, channel_attention])\n",
        "\n",
        "# Spatial attention module\n",
        "avg_pool = Lambda(lambda x: tf.keras.backend.mean(x, axis=3, keepdims=True))(x)\n",
        "max_pool = Lambda(lambda x: tf.keras.backend.max(x, axis=3, keepdims=True))(x)\n",
        "concat = Concatenate(axis=3)([avg_pool, max_pool])\n",
        "spatial_attention = Conv2D(filters=1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n",
        "#spatial_attention = Lambda(lambda x: x / tf.reduce_max(x))(spatial_attention)\n",
        "x = Multiply()([x, spatial_attention])\n",
        "\n",
        "# Build the model architecture\n",
        "input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
        "x = vgg_model(input_tensor)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)  # Add dropout regularization\n",
        "x = Dense(N_CLASSES, activation='softmax', kernel_regularizer=l2(0.01), name='output')(x)\n",
        "model_vgg3 = Model(inputs=input_tensor, outputs=x)\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model_vgg3.compile(\n",
        "    optimizer=Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy', # binary_crossentropy for two class\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Learning Rate Scheduling\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, min_lr=1e-7)\n",
        "\n",
        "# Fine-tune the model\n",
        "EPOCHS = 50\n",
        "history_model3 = model_vgg3.fit(\n",
        "    train_generator,\n",
        "    epochs=EPOCHS,\n",
        "    validation_data=valid_generator,\n",
        "    callbacks=[reduce_lr],\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model_vgg3.evaluate(\n",
        "    test_generator,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)"
      ]
    }
  ]
}